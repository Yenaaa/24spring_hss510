{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "T6APIdUNkbLM"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yenaaa/24spring_hss510/blob/main/Embeddings_Apr24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **HSS 510 Guide Coding: Embeddings**\n",
        "\n",
        "### **2024 Apr 24, Taegyoon Kim**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "A0Nh--a0bIGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topics**\n",
        "- Training custom embeddings (built based on [Burt Monroe's tutorial](https://colab.research.google.com/drive/1eSzd2z5B3CDeTxpdMXCIh3bm1L-gYzCr?usp=sharing#scrollTo=3R_ZkQp331VX))\n",
        "- Two models\n",
        "  - Word2Vec\n",
        "  - FastText\n"
      ],
      "metadata": {
        "id": "5OLmPOUibbes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Estimating embeddings on a corpus from the House of Common in Britain**\n",
        "\n",
        "- One of the datasets available via Cornell Conversational Analysis Toolkit (ConvoKit)\n",
        "- A collections of questions and answers from parliamentary question periods in the British House of Commons from May 1979 to December 2016 (433,787 statements)"
      ],
      "metadata": {
        "id": "oTdwH-DQjnwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting data via `convokit` and pre-processing"
      ],
      "metadata": {
        "id": "T6APIdUNkbLM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtiK8ZXLR6o2"
      },
      "source": [
        "!pip3 install convokit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-XNgRitMovA"
      },
      "source": [
        "- Download the corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-2H6qf2SRxL"
      },
      "source": [
        "from convokit import Corpus, download"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIDud31zMirS"
      },
      "source": [
        "- `nltk` tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP-wIrM4VKM7"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWHShYy_S87X",
        "outputId": "29e16705-9488-412a-dd41-fa24379a9430"
      },
      "source": [
        "corpus = Corpus(filename = download(\"parliament-corpus\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading parliament-corpus to /root/.convokit/downloads/parliament-corpus\n",
            "Downloading parliament-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/parliament-corpus/parliament-corpus.zip (368.2MB)... Done\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "default_backend: mem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qH4C22lTN-e",
        "outputId": "ec6261f7-3fa9-4192-ad01-a8c85d8602bb"
      },
      "source": [
        "corpus.print_summary_stats()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Speakers: 1978\n",
            "Number of Utterances: 433787\n",
            "Number of Conversations: 216894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTBmXmG9Mwcw"
      },
      "source": [
        "- Let's look at the first utterance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flNbncFyT5m6",
        "outputId": "f28bc001-7dab-4bf2-facf-beffbba53335"
      },
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print(utt.text)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I thank the Minister for his response . He will be aware that the Northern Ireland Policing Board and the Chief Constable are concerned about a possible reduction in the police budget in the forthcoming financial year , and that there are increasing pressures on the budget as a result of policing the past , the ongoing inquiries , and the cost of the legal advice that the police need to secure in order to participate in them . However , does he agree that it is right that the Government provide adequate funding for the ordinary policing in the community that tackles all the matters that concern the people of Northern Ireland ? Does he accept that there should not be a reduction in the police budget , given the increasing costs of the inquiries that I have mentioned ? Will the Government do something to reduce the cost of the inquiries , and ensure that adequate policing is provided for all the victims of crime in Northern Ireland ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp_8lT46Mzj1"
      },
      "source": [
        "- Let's look at how the tokenizer works for the first utterance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsH8k9vF5LQz",
        "outputId": "e63e7b75-e700-47b7-c76f-94b59e5405ee"
      },
      "source": [
        "for utt in corpus.iter_utterances():\n",
        "    print( [word_tokenize(t) for t in sent_tokenize(utt.text)])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'thank', 'the', 'Minister', 'for', 'his', 'response', '.'], ['He', 'will', 'be', 'aware', 'that', 'the', 'Northern', 'Ireland', 'Policing', 'Board', 'and', 'the', 'Chief', 'Constable', 'are', 'concerned', 'about', 'a', 'possible', 'reduction', 'in', 'the', 'police', 'budget', 'in', 'the', 'forthcoming', 'financial', 'year', ',', 'and', 'that', 'there', 'are', 'increasing', 'pressures', 'on', 'the', 'budget', 'as', 'a', 'result', 'of', 'policing', 'the', 'past', ',', 'the', 'ongoing', 'inquiries', ',', 'and', 'the', 'cost', 'of', 'the', 'legal', 'advice', 'that', 'the', 'police', 'need', 'to', 'secure', 'in', 'order', 'to', 'participate', 'in', 'them', '.'], ['However', ',', 'does', 'he', 'agree', 'that', 'it', 'is', 'right', 'that', 'the', 'Government', 'provide', 'adequate', 'funding', 'for', 'the', 'ordinary', 'policing', 'in', 'the', 'community', 'that', 'tackles', 'all', 'the', 'matters', 'that', 'concern', 'the', 'people', 'of', 'Northern', 'Ireland', '?'], ['Does', 'he', 'accept', 'that', 'there', 'should', 'not', 'be', 'a', 'reduction', 'in', 'the', 'police', 'budget', ',', 'given', 'the', 'increasing', 'costs', 'of', 'the', 'inquiries', 'that', 'I', 'have', 'mentioned', '?'], ['Will', 'the', 'Government', 'do', 'something', 'to', 'reduce', 'the', 'cost', 'of', 'the', 'inquiries', ',', 'and', 'ensure', 'that', 'adequate', 'policing', 'is', 'provided', 'for', 'all', 'the', 'victims', 'of', 'crime', 'in', 'Northern', 'Ireland', '?']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUrQAcZ6U6Bw"
      },
      "source": [
        "- Generate the sentence tokens, and the word tokens within them. This took ~ 5 minutes, given 430,000 utterances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEEW9D1C9NAo"
      },
      "source": [
        "sents = []\n",
        "for utt in corpus.iter_utterances():\n",
        "    sents.append([word_tokenize(t) for t in sent_tokenize(utt.text)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-THHCld6odo",
        "outputId": "46bf7823-e52a-45d6-8c1e-3e454c8c3b4b"
      },
      "source": [
        "type(sents)\n",
        "type(sents[0])\n",
        "type(sents[0][0])\n",
        "type(sents[0][0])\n",
        "\n",
        "for i in sents[0]:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'thank', 'the', 'Minister', 'for', 'his', 'response', '.']\n",
            "['He', 'will', 'be', 'aware', 'that', 'the', 'Northern', 'Ireland', 'Policing', 'Board', 'and', 'the', 'Chief', 'Constable', 'are', 'concerned', 'about', 'a', 'possible', 'reduction', 'in', 'the', 'police', 'budget', 'in', 'the', 'forthcoming', 'financial', 'year', ',', 'and', 'that', 'there', 'are', 'increasing', 'pressures', 'on', 'the', 'budget', 'as', 'a', 'result', 'of', 'policing', 'the', 'past', ',', 'the', 'ongoing', 'inquiries', ',', 'and', 'the', 'cost', 'of', 'the', 'legal', 'advice', 'that', 'the', 'police', 'need', 'to', 'secure', 'in', 'order', 'to', 'participate', 'in', 'them', '.']\n",
            "['However', ',', 'does', 'he', 'agree', 'that', 'it', 'is', 'right', 'that', 'the', 'Government', 'provide', 'adequate', 'funding', 'for', 'the', 'ordinary', 'policing', 'in', 'the', 'community', 'that', 'tackles', 'all', 'the', 'matters', 'that', 'concern', 'the', 'people', 'of', 'Northern', 'Ireland', '?']\n",
            "['Does', 'he', 'accept', 'that', 'there', 'should', 'not', 'be', 'a', 'reduction', 'in', 'the', 'police', 'budget', ',', 'given', 'the', 'increasing', 'costs', 'of', 'the', 'inquiries', 'that', 'I', 'have', 'mentioned', '?']\n",
            "['Will', 'the', 'Government', 'do', 'something', 'to', 'reduce', 'the', 'cost', 'of', 'the', 'inquiries', ',', 'and', 'ensure', 'that', 'adequate', 'policing', 'is', 'provided', 'for', 'all', 'the', 'victims', 'of', 'crime', 'in', 'Northern', 'Ireland', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxTHaAgPVNho"
      },
      "source": [
        "* That's the first document/utterance, a list of lists (each sentence is a list of tokens)\n",
        "* That means sents is organized as a list of lists of lists\n",
        "* Word2Vec wants a list of lists (the tokens by sentence, without distinguishing between the utterances in which they are used).\n",
        "* So, we flatten the list (to a list of sentences, each a list of tokens)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqZrugUSN5cD"
      },
      "source": [
        "flat_sents_list = [sentence for utt in sents for sentence in utt] # for every utterance, loop over its sentences and add them to the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Fi0uuGzW4fh",
        "outputId": "7ed92d9c-2606-4ab0-b14a-d81aacad50ab"
      },
      "source": [
        "print(len(flat_sents_list))\n",
        "type(flat_sents_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1354489\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read the preprocessed list (of lists)"
      ],
      "metadata": {
        "id": "7YWGpx2ol480"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The pre-processed corpus, \"house_commons_speech.pkl\", available via [this link](https://drive.google.com/file/d/1KU5pukWUTWfsJqru79UgMyOnOPjpUjYd/view?usp=share_link)"
      ],
      "metadata": {
        "id": "YAC8n4SnrhZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/house_commons_speech.pkl', 'rb') as file:\n",
        "    flat_sents_list = pickle.load(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GlJaAGHfI9d",
        "outputId": "1a6d8e21-83c6-4830-eb28-32e877fb0ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(flat_sents_list)) # approximately 1.4M sentences (a list of sentences)\n",
        "print(flat_sents_list[0]) # each sentence is recorded as a list of words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWrjA_a0pvPn",
        "outputId": "12a43a1a-19b4-4ed4-b5fc-ea027d02c2c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1354489\n",
            "['I', 'thank', 'the', 'Minister', 'for', 'his', 'response', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(flat_sents_list[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qxzsk6CpgzM",
        "outputId": "dbb346c4-4c24-4fa3-f01d-420469d645e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'thank', 'the', 'Minister', 'for', 'his', 'response', '.']\n",
            "['He', 'will', 'be', 'aware', 'that', 'the', 'Northern', 'Ireland', 'Policing', 'Board', 'and', 'the', 'Chief', 'Constable', 'are', 'concerned', 'about', 'a', 'possible', 'reduction', 'in', 'the', 'police', 'budget', 'in', 'the', 'forthcoming', 'financial', 'year', ',', 'and', 'that', 'there', 'are', 'increasing', 'pressures', 'on', 'the', 'budget', 'as', 'a', 'result', 'of', 'policing', 'the', 'past', ',', 'the', 'ongoing', 'inquiries', ',', 'and', 'the', 'cost', 'of', 'the', 'legal', 'advice', 'that', 'the', 'police', 'need', 'to', 'secure', 'in', 'order', 'to', 'participate', 'in', 'them', '.']\n",
            "['However', ',', 'does', 'he', 'agree', 'that', 'it', 'is', 'right', 'that', 'the', 'Government', 'provide', 'adequate', 'funding', 'for', 'the', 'ordinary', 'policing', 'in', 'the', 'community', 'that', 'tackles', 'all', 'the', 'matters', 'that', 'concern', 'the', 'people', 'of', 'Northern', 'Ireland', '?']\n",
            "['Does', 'he', 'accept', 'that', 'there', 'should', 'not', 'be', 'a', 'reduction', 'in', 'the', 'police', 'budget', ',', 'given', 'the', 'increasing', 'costs', 'of', 'the', 'inquiries', 'that', 'I', 'have', 'mentioned', '?']\n",
            "['Will', 'the', 'Government', 'do', 'something', 'to', 'reduce', 'the', 'cost', 'of', 'the', 'inquiries', ',', 'and', 'ensure', 'that', 'adequate', 'policing', 'is', 'provided', 'for', 'all', 'the', 'victims', 'of', 'crime', 'in', 'Northern', 'Ireland', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Tvv8w9IHiX"
      },
      "source": [
        "import gensim # library for various NLP tasks including LDA, Word2Vec, etc.\n",
        "from gensim.models import Word2Vec # import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimate Word2Vec embeddings"
      ],
      "metadata": {
        "id": "gFh3B6Dkr4w_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwP_CUH_V77e"
      },
      "source": [
        "- Estimate the word2vec model\n",
        "  - Here we the default dimensionality of 100 (`vector_size`)\n",
        "  - We set the context `window` at 5\n",
        "  - The `min_count` of token frequency defaults to 1, but I set it at 5\n",
        "  - For parallelization, set `workers` > 1\n",
        "  - `sg` is for skip-gram if 1; otherwise CBOW (default 0)\n",
        "  - `negative` is for negative sampling (in SGNS) if > 0 (specifies how many \"noise words\" should be drawn (usually between 5--20))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SntK0q0KfHm"
      },
      "source": [
        "model_w5 = Word2Vec(sentences = flat_sents_list, # CBOW\n",
        "                    vector_size = 100,\n",
        "                    window = 5,\n",
        "                    min_count = 5,\n",
        "                    workers = 1)\n",
        "model_w5.save(\"w5_word2vec.model\") # save the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w5 = Word2Vec.load(\"w5_word2vec.model\")"
      ],
      "metadata": {
        "id": "rBD5IGLF2DpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the embeddings"
      ],
      "metadata": {
        "id": "eHpsEQWn0idH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Get word vectors and compare"
      ],
      "metadata": {
        "id": "biAL7gkWAGAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drug = model_w5.wv[\"drug\"]\n",
        "medicine = model_w5.wv[\"medicine\"]\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "dot(drug, medicine)/(norm(medicine)*norm(medicine))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3-Yt5E3AFaS",
        "outputId": "966ea99a-1181-4181-da77-3d8e6be47d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.82365096"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFaqh8DuNWWi"
      },
      "source": [
        "* Now let's see what words are near each other\n",
        "* We see that \"Health\" seems to be close to other words that might appear in a Bill name or ministerial title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvXzrgtdPNkG",
        "outputId": "122d7c44-2375-4480-876d-f76650b04d51"
      },
      "source": [
        "model_w5.wv.most_similar(\"Health\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Employment', 0.7862419486045837),\n",
              " ('Prison', 0.7468486428260803),\n",
              " ('Forensic', 0.722485363483429),\n",
              " ('Insolvency', 0.7215125560760498),\n",
              " ('Hygiene', 0.7053036689758301),\n",
              " ('Tribunals', 0.7050362229347229),\n",
              " ('Education', 0.7035816311836243),\n",
              " ('Arbitration', 0.698085606098175),\n",
              " ('Admissions', 0.6963907480239868),\n",
              " ('Transport', 0.6780714392662048)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgCkfw5HNlpt"
      },
      "source": [
        "* Whereas \"health\" appears near some semi-antonyms, \"handicap\" \"illness\"\n",
        "* Some words in the same \"semantic field\" like \"ambulance\" and one that is probably a type that appears in the same contexts, \"heath\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hArVKq6wQAD1",
        "outputId": "fb3b4276-fb59-4f70-acb5-287ff8fc92a6"
      },
      "source": [
        "model_w5.wv.most_similar(\"health\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('handicap', 0.6810250878334045),\n",
              " ('heath', 0.6683714985847473),\n",
              " ('probation', 0.6625848412513733),\n",
              " ('Connexions', 0.6424758434295654),\n",
              " ('illness', 0.6394876837730408),\n",
              " ('library', 0.6013481616973877),\n",
              " ('fire', 0.582091212272644),\n",
              " ('domiciliary', 0.5721033215522766),\n",
              " ('healthcare', 0.5678114295005798),\n",
              " ('111', 0.5606209635734558)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_w5.wv.most_similar(\"immigrants\") # synonyms or same semantic fields (like topics)?"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQmH3x701ofJ",
        "outputId": "d1ec544f-bcd5-4cce-dae9-fad5260e59e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('encampments', 0.7111466526985168),\n",
              " ('migrants', 0.6910161972045898),\n",
              " ('downloading', 0.6780105233192444),\n",
              " ('criminals', 0.6467640995979309),\n",
              " ('illegally', 0.6046491861343384),\n",
              " ('refugees', 0.6017322540283203),\n",
              " ('logging', 0.6014271378517151),\n",
              " ('terrorists', 0.5980145335197449),\n",
              " ('nationals', 0.5940695405006409),\n",
              " ('gangs', 0.5936878323554993)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54KJAKL0OD5Q"
      },
      "source": [
        "* Vector arithmetic on embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o4g1AzaRKa8",
        "outputId": "319e11f6-48e1-4e25-a1fb-2131dcc6af90"
      },
      "source": [
        "model_w5.wv.most_similar(positive = [\"Thatcher\", \"liberal\"], negative = [\"conservative\"]) # Thatcher - conservative + liberal = ?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Tony', 0.5067369341850281),\n",
              " ('Blair', 0.5006837844848633),\n",
              " ('Porter', 0.48702752590179443),\n",
              " ('Mugabe', 0.483844518661499),\n",
              " ('Tsvangirai', 0.48103344440460205),\n",
              " ('Hermon', 0.4795630872249603),\n",
              " ('Tikkoo', 0.4764277935028076),\n",
              " ('Gandhi', 0.47223103046417236),\n",
              " ('Attlee', 0.46526196599006653),\n",
              " ('Botha', 0.4642172157764435)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-GnHgW-QLwp",
        "outputId": "25e1b200-1d1c-4750-e124-82c0d3abe084"
      },
      "source": [
        "model_w5.wv.most_similar(positive = [\"doctor\", \"female\"], negative = [\"male\"]) # doctor - male + female = ?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('consultant', 0.6827680468559265),\n",
              " ('solicitor', 0.6116743087768555),\n",
              " ('nurse', 0.6089175343513489),\n",
              " ('surgeon', 0.599676251411438),\n",
              " ('vet', 0.5974711179733276),\n",
              " ('GP', 0.589309573173523),\n",
              " ('policeman', 0.589016318321228),\n",
              " ('lawyer', 0.5823254585266113),\n",
              " ('scientist', 0.5728553533554077),\n",
              " ('woman', 0.5716930031776428)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering embeddings"
      ],
      "metadata": {
        "id": "62BgYZ-20l2C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLqDhhELSu7v"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lNXQUO6hPa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1aac98-07bf-4467-86a8-0f78fbfae7ef"
      },
      "source": [
        "vectors_w5 = np.asarray(model_w5.wv.vectors) # extract the words & their vectors, as numpy arrays"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.0596823e-02,  1.7236420e+00,  2.3940232e+00, ...,\n",
              "         1.5053058e+00,  7.1751916e-01, -6.5607935e-01],\n",
              "       [-1.5499238e+00,  2.6047045e-01,  4.3965790e-01, ...,\n",
              "         4.3094710e-01,  1.1707065e+00,  1.3633395e+00],\n",
              "       [ 2.6354363e+00,  1.1808308e+00,  1.5802506e+00, ...,\n",
              "         2.4170867e-01, -1.5850221e+00,  3.6300176e-01],\n",
              "       ...,\n",
              "       [ 2.5155351e-03,  3.4930080e-02, -3.2755081e-02, ...,\n",
              "        -7.3378660e-02,  5.2573767e-02, -7.6513782e-02],\n",
              "       [-3.0044798e-02,  8.5736960e-02, -1.1937888e-01, ...,\n",
              "        -2.3653543e-02,  9.4190761e-02,  2.1030908e-02],\n",
              "       [-5.1110145e-02,  1.0469507e-01,  5.7784956e-02, ...,\n",
              "         4.9279374e-04,  6.3367225e-02,  6.6300230e-03]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAB-cDsGiIRl",
        "outputId": "35b6face-68dc-4ee5-880a-828cd0f76673"
      },
      "source": [
        "vectors_w5.shape # number of words in the vocabulary X dimension of the embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41235, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "LZotbWIIkAx3",
        "outputId": "be514ed7-cae8-40cd-d8a6-ef6f94187a45"
      },
      "source": [
        "kmeans_w5_20 = KMeans(n_clusters = 20) # initializes a KMeans clustering algorithm with 20 clusters\n",
        "kmeans_w5_20.fit(vectors_w5) # fits the KMeans algorithm to the word embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KMeans(n_clusters=20)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=20)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3-8jBkbkmqO",
        "outputId": "af4e7da2-5770-42ab-989d-5a459b671bce"
      },
      "source": [
        "kmeans_w5_20.labels_ # the cluster labels for each word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kmeans_w5_20.shape)\n",
        "print(set(kmeans_w5_20.labels_))"
      ],
      "metadata": {
        "id": "KRtC4ze1ecVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNz43RmGn3Au",
        "outputId": "aab8b6ce-58e2-4db2-ed24-7029869baa77"
      },
      "source": [
        "kmeans_w5_20.cluster_centers_ # the centroids of the clusters"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.11185617,  0.00788689, -0.9114809 , ..., -0.15981878,\n",
              "         0.7854867 ,  0.1168226 ],\n",
              "       [-1.2406604 , -0.01420841,  0.09974949, ...,  0.9221337 ,\n",
              "        -0.36265895,  1.0386531 ],\n",
              "       [-0.7884922 , -1.4349527 ,  0.69250095, ...,  0.7765105 ,\n",
              "         0.0176113 , -0.65730536],\n",
              "       ...,\n",
              "       [-0.1882824 , -0.29087692,  0.906218  , ...,  0.30241567,\n",
              "        -0.15389001,  0.46879336],\n",
              "       [ 0.2298541 ,  0.21578324,  0.45012796, ...,  0.3207534 ,\n",
              "        -0.17909549,  0.17485279],\n",
              "       [ 0.88356346,  0.31486085,  0.30560428, ...,  1.0943521 ,\n",
              "         0.31699774, -0.0038153 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(kmeans_w5_20.cluster_centers_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRuJ6wtqemoX",
        "outputId": "a48163f9-c3b6-4bab-f345-a41be63b1cc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hux8G39TnWqQ",
        "outputId": "56e984a0-4e2f-4f74-bb9a-7aa67e7a9573"
      },
      "source": [
        "model_w5.wv.most_similar([kmeans_w5_20.cluster_centers_[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Chester', 0.8924877047538757),\n",
              " ('Bristol', 0.8877990245819092),\n",
              " ('Southampton', 0.8869059681892395),\n",
              " ('Preston', 0.8842563033103943),\n",
              " ('Warrington', 0.8814603686332703),\n",
              " ('Swindon', 0.8802078366279602),\n",
              " ('Luton', 0.8785504698753357),\n",
              " ('Rochdale', 0.8696098327636719),\n",
              " ('Aberdeen', 0.8686888813972473),\n",
              " ('Durham', 0.8617432713508606)]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG1Ik55Mo-29",
        "outputId": "104119e0-6a8a-47ad-b212-e64a4ed13dd0"
      },
      "source": [
        "for k in range(20):\n",
        "  print(model_w5.wv.most_similar([kmeans_w5_20.cluster_centers_[k]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Chester', 0.8924877047538757), ('Bristol', 0.8877990245819092), ('Southampton', 0.8869059681892395), ('Preston', 0.8842563033103943), ('Warrington', 0.8814603686332703), ('Swindon', 0.8802078366279602), ('Luton', 0.8785504698753357), ('Rochdale', 0.8696098327636719), ('Aberdeen', 0.8686888813972473), ('Durham', 0.8617432713508606)]\n",
            "[('blocked', 0.8361625075340271), ('overtaken', 0.828234076499939), ('attacked', 0.8226117491722107), ('observed', 0.8142275810241699), ('demanded', 0.8097203969955444), ('overruled', 0.8005773425102234), ('challenged', 0.7976050972938538), ('promoted', 0.7971864938735962), ('overseen', 0.7916258573532104), ('accepted', 0.78579181432724)]\n",
            "[('35', 0.9265395402908325), ('45', 0.9235835671424866), ('55', 0.9186977744102478), ('40', 0.9154132604598999), ('70', 0.9150171279907227), ('38', 0.9129404425621033), ('60', 0.9118229150772095), ('34', 0.9100478291511536), ('200', 0.9096612930297852), ('52', 0.9096236228942871)]\n",
            "[('observations', 0.7959889769554138), ('views', 0.7767202258110046), ('comments', 0.775723397731781), ('criticism', 0.7673346996307373), ('strictures', 0.7525913119316101), ('remarks', 0.7413244843482971), ('suggestion', 0.7257604598999023), ('replies', 0.7213351726531982), ('assertions', 0.7097830176353455), ('pronouncements', 0.6999098062515259)]\n",
            "[('bill', 0.7433518171310425), ('repayment', 0.7205885648727417), ('earnings', 0.7153991460800171), ('income', 0.7059094309806824), ('revenue', 0.6998578310012817), ('rebates', 0.6945285201072693), ('ceiling', 0.6941359639167786), ('20p', 0.6926637291908264), ('repayments', 0.6897851228713989), ('allowances', 0.6872172951698303)]\n",
            "[('youngsters', 0.8046385645866394), ('adults', 0.7919157147407532), ('women', 0.7845722436904907), ('children', 0.7763471007347107), ('mothers', 0.7637438178062439), ('GPs', 0.7581156492233276), ('volunteers', 0.7559811472892761), ('inmates', 0.755055844783783), ('mums', 0.7371871471405029), ('prisoners', 0.7345691323280334)]\n",
            "[('foolish', 0.7228168249130249), ('undesirable', 0.7184860706329346), ('unlikely', 0.7173905372619629), ('unhelpful', 0.7044678926467896), ('ineffective', 0.6985698938369751), ('clever', 0.6975135207176208), ('unusual', 0.6959223747253418), ('prevalent', 0.695488691329956), ('disappointing', 0.6919050216674805), ('naive', 0.6873503923416138)]\n",
            "[('strengthening', 0.6528626084327698), ('matching', 0.6407714486122131), ('enhancing', 0.6153361797332764), ('accounting', 0.6056122183799744), ('developing', 0.598131000995636), ('limiting', 0.5944029092788696), ('pooling', 0.5935974717140198), ('improving', 0.584365963935852), ('devising', 0.5804025530815125), ('simplifying', 0.5799539685249329)]\n",
            "[('contribute', 0.8034828901290894), ('pursue', 0.7795873284339905), ('apply', 0.7693638801574707), ('extend', 0.7676607966423035), ('adhere', 0.7634923458099365), ('deliver', 0.7593336701393127), ('retain', 0.7588068246841431), ('dictate', 0.756733238697052), ('respond', 0.7503563165664673), ('organise', 0.7464197874069214)]\n",
            "[('structures', 0.7567123770713806), ('instruments', 0.7027268409729004), ('processes', 0.6953827738761902), ('provisions', 0.6937291622161865), ('procedures', 0.6934908032417297), ('institutions', 0.6934449076652527), ('protections', 0.6841621994972229), ('framework', 0.6772921681404114), ('arrangements', 0.6757287979125977), ('considerations', 0.6715883016586304)]\n",
            "[('August', 0.8519337773323059), ('September', 0.8442701101303101), ('October', 0.8354611992835999), ('July', 0.8149120211601257), ('March', 0.8102110624313354), ('June', 0.8087335228919983), ('November', 0.8033862113952637), ('December', 0.7958714962005615), ('1999', 0.7907024025917053), ('January', 0.7855268716812134)]\n",
            "[('prepared', 0.8297872543334961), ('able', 0.7691146731376648), ('willing', 0.7561920285224915), ('can', 0.7411985993385315), ('attempting', 0.7368277907371521), ('obliged', 0.7267596125602722), ('unable', 0.7140782475471497), ('trying', 0.7068968415260315), ('seeking', 0.7017920613288879), ('will', 0.6987099051475525)]\n",
            "[('Hugh', 0.9083084464073181), ('Trevor', 0.8886339664459229), ('Anthony', 0.8871510028839111), ('Nick', 0.8835299015045166), ('Adam', 0.8831082582473755), ('Tim', 0.8803540468215942), ('Allan', 0.877700924873352), ('Steve', 0.874169647693634), ('Simon', 0.8729806542396545), ('Edwards', 0.8728720545768738)]\n",
            "[('Management', 0.8956630229949951), ('Advisory', 0.8759854435920715), ('Standards', 0.8688695430755615), ('Consumer', 0.854464590549469), ('Physical', 0.8529571890830994), ('Technology', 0.851259171962738), ('Campaign', 0.8499867916107178), ('Waste', 0.8439190983772278), ('Teacher', 0.8428719639778137), ('Intelligence', 0.8404397368431091)]\n",
            "[('remind', 0.8659723401069641), ('suggest', 0.7944709658622742), ('warn', 0.7791071534156799), ('appreciate', 0.7701749205589294), ('acknowledge', 0.7673733830451965), ('tell', 0.7640095353126526), ('inform', 0.7633006572723389), ('reassure', 0.7601902484893799), ('concede', 0.7585314512252808), ('know', 0.7544363737106323)]\n",
            "[('Boroughbridge', 0.9047168493270874), ('mumps', 0.894402027130127), ('saline', 0.8908671736717224), ('Rebekah', 0.8829658627510071), ('Techniques', 0.8809702396392822), ('Shame', 0.8780004978179932), ('Diolch', 0.8776659965515137), ('Stefan', 0.876236617565155), ('Withdraw', 0.8743399381637573), ('K.', 0.8691477179527283)]\n",
            "[('Ukraine', 0.871771514415741), ('Russia', 0.871104896068573), ('Serbia', 0.8647453784942627), ('Libya', 0.8541874885559082), ('Turkey', 0.8518019914627075), ('Croatia', 0.8453218340873718), ('Syria', 0.8407172560691833), ('Lebanon', 0.8368451595306396), ('Egypt', 0.8320853114128113), ('Poland', 0.8317043781280518)]\n",
            "[('pulled', 0.8479074835777283), ('kicked', 0.8436936140060425), ('locked', 0.8399049639701843), ('turned', 0.8280855417251587), ('pushed', 0.8248148560523987), ('thrown', 0.8144311308860779), ('pulling', 0.8141469359397888), ('turns', 0.8015474677085876), ('stepping', 0.7868776321411133), ('locking', 0.7820380330085754)]\n",
            "[('vandalism', 0.8229596614837646), ('brutality', 0.8166724443435669), ('intimidation', 0.7857289910316467), ('harassment', 0.7755539417266846), ('killings', 0.7626418471336365), ('disturbance', 0.760604202747345), ('carnage', 0.7582669854164124), ('hideous', 0.7572288513183594), ('rioting', 0.7548805475234985), ('conspiracy', 0.7509663105010986)]\n",
            "[('timber', 0.8203099370002747), ('engines', 0.8142133951187134), ('furniture', 0.7992383241653442), ('manufacture', 0.7861728668212891), ('components', 0.7768006324768066), ('diesel', 0.7715683579444885), ('liquid', 0.7691231369972229), ('biodiesel', 0.7583820223808289), ('materials', 0.7579928636550903), ('chemicals', 0.7533720135688782)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL5JDR3LOjv6"
      },
      "source": [
        "## FastText embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW8cbVrzHFeQ"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4O2LmAnHbAF"
      },
      "source": [
        "modelf_w5 = FastText(sentences = flat_sents_list,\n",
        "                     vector_size = 100,\n",
        "                     window = 5,\n",
        "                     min_count = 5,\n",
        "                     workers = 1)\n",
        "modelf_w5.save(\"w5_fasttext.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelf_w5 = Word2Vec.load(\"w5_fasttext.model\")"
      ],
      "metadata": {
        "id": "c7T4mNsyAqGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_w5.wv.most_similar(\"appple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "nVCPs-zF_ob5",
        "outputId": "e383b93a-ce87-407a-deb0-21e250e1fcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'appple' not present in vocabulary\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-447c4772c923>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_w5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"appple\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'appple' not present in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelf_w5.wv.most_similar(\"appple\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wg6roy4-wDO",
        "outputId": "40f692f6-6ea0-4e8b-f27b-2397328d604e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apple', 0.9685217142105103),\n",
              " ('grapple', 0.8550187349319458),\n",
              " ('Apple', 0.8018854856491089),\n",
              " ('axle', 0.7925280332565308),\n",
              " ('temple', 0.7810781002044678),\n",
              " ('remnant', 0.7659269571304321),\n",
              " ('applicable', 0.758592426776886),\n",
              " ('apprise', 0.754023551940918),\n",
              " ('salient', 0.7537946105003357),\n",
              " ('fate', 0.7521312832832336)]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_w5.wv.most_similar(\"labor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAoOLm8W-3Mv",
        "outputId": "adbd12ac-f646-467b-b064-d4d1e6e2ea64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.37449843,  0.83503675, -0.070547  ,  1.8360862 ,  1.1604441 ,\n",
              "       -4.015069  ,  3.0558116 , -2.759304  , -1.2821283 , -3.1344988 ,\n",
              "       -4.084456  , -0.11581492, -0.79035634,  0.02368222, -1.3907524 ,\n",
              "        2.073515  , -2.4636502 , -2.4736023 ,  1.2599984 ,  0.55936354,\n",
              "       -1.4227118 , -0.86802983, -0.80151856,  1.9569576 ,  0.96252275,\n",
              "       -2.5417376 , -1.4038805 ,  3.097222  , -2.670208  , -3.4097917 ,\n",
              "        2.722994  , -3.04929   ,  0.71016914, -1.2947173 ,  2.4244049 ,\n",
              "        1.1838018 , -0.24309714, -0.11585805, -0.93109643,  2.7839098 ,\n",
              "        1.6343476 ,  0.0043371 , -0.18669774, -1.3995315 ,  3.6639977 ,\n",
              "        3.4438179 ,  0.14050567,  0.08193759, -2.0734231 ,  1.3409727 ,\n",
              "       -1.2985497 , -2.1335106 , -2.228774  ,  1.456587  ,  0.27012268,\n",
              "       -1.0316004 , -1.9174016 ,  3.6536    , -2.8048084 , -1.6629081 ,\n",
              "        2.0278876 , -1.0374986 , -1.9126713 ,  1.4844654 ,  0.31577754,\n",
              "       -0.8683158 ,  2.723957  , -2.574666  , -3.2242434 , -1.988502  ,\n",
              "       -0.53694236, -3.1618638 , -1.9937279 ,  1.2686899 ,  1.673212  ,\n",
              "       -0.3507551 ,  0.36369982, -0.5355074 ,  0.02990297,  4.0986414 ,\n",
              "       -1.3479257 ,  3.7327528 ,  0.86428535, -2.1561196 ,  0.6935075 ,\n",
              "       -1.1671584 , -2.7561367 , -0.5175063 , -1.2529289 , -0.0962916 ,\n",
              "       -1.8377463 ,  3.5380461 ,  3.7140756 , -1.2655984 ,  1.5673304 ,\n",
              "       -0.50622207,  3.895758  ,  3.7701812 ,  2.1321354 , -2.4293435 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelf_w5.wv.most_similar(\"labor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iyxfAFN_mFZ",
        "outputId": "2ba34677-8136-4a57-a6a7-fb75ce9fd725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('laboratory', 0.7762442827224731),\n",
              " ('labourer', 0.7587530016899109),\n",
              " ('lab', 0.7357218861579895),\n",
              " ('labours', 0.6982717514038086),\n",
              " ('collaborating', 0.6888777017593384),\n",
              " ('labouring', 0.6887030005455017),\n",
              " ('labour', 0.6834583878517151),\n",
              " ('labs', 0.6768965125083923),\n",
              " ('collaborate', 0.6719852685928345),\n",
              " ('computer', 0.665510892868042)]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    }
  ]
}